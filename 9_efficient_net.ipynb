{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef53f0b-f1bb-4291-82c1-b56f48a0a922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.hub import load_state_dict_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e57352-03cb-407b-b7c0-11de1276ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.cv_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "96165136-27e9-4f51-87ac-31a45a9b8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=4):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels // reduction, 1)\n",
    "        self.conv2 = nn.Conv2d(in_channels // reduction, in_channels, 1)\n",
    "        self.silu = nn.SiLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        se = self.global_avg_pool(x)\n",
    "        se = self.silu(self.conv1(se))\n",
    "        se = F.sigmoid(self.conv2(se))\n",
    "        x = x * se\n",
    "        return x\n",
    "\n",
    "class ExpandBlock(nn.Module):\n",
    "    def __init__(self, in_channels, expand_ratio):\n",
    "        super(ExpandBlock, self).__init__()\n",
    "        hidden_dim = max(1, int(in_channels * expand_ratio))\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.expand(x)\n",
    "\n",
    "class DepthWiseConv(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size, stride):\n",
    "        super(DepthWiseConv, self).__init__()\n",
    "        self.depth_wise = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=in_channels, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depth_wise(x)\n",
    "        return x\n",
    "\n",
    "class ProjectConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ProjectConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio):\n",
    "        super(MBConv, self).__init__()\n",
    "        self.use_residual = in_channels == out_channels and stride == 1\n",
    "        \n",
    "        hidden_dim = max(1, int(in_channels * expand_ratio))\n",
    "        reduction = max(1, int(hidden_dim * se_ratio))\n",
    "        # print(hidden_dim, reduction)\n",
    "        self.expand = None\n",
    "        if expand_ratio != 1:\n",
    "            self.expand = ExpandBlock(in_channels, expand_ratio)\n",
    "\n",
    "        self.depth_wise = DepthWiseConv(hidden_dim, kernel_size, stride)\n",
    "        self.se = SEBlock(hidden_dim, reduction)\n",
    "        self.projection = ProjectConv(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        if self.expand is not None:\n",
    "            x = self.expand(x)\n",
    "\n",
    "        x = self.depth_wise(x)\n",
    "        x = self.se(x)\n",
    "        x = self.projection(x)\n",
    "        \n",
    "        if self.use_residual:\n",
    "            x += identity\n",
    "\n",
    "        return x\n",
    "\n",
    "class Efficient_Net_B0(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Efficient_Net_B0, self).__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio, n_repeats\n",
    "        self.conv1 = self._make_layers(32, 16, 3, 1, 1, 0.25, 1)\n",
    "        self.conv2 = self._make_layers(16, 24, 3, 2, 6, 0.25, 2)\n",
    "        self.conv3 = self._make_layers(24, 40, 5, 2, 6, 0.25, 2)\n",
    "        self.conv4 = self._make_layers(40, 80, 3, 2, 6, 0.25, 3)\n",
    "        self.conv5 = self._make_layers(80, 112, 5, 1, 6, 0.25, 3)\n",
    "        self.conv6 = self._make_layers(112, 192, 5, 2, 6, 0.25, 4)\n",
    "        self.conv7 = self._make_layers(192, 320, 3, 1, 6, 0.25, 1)\n",
    "\n",
    "        self.head_conv = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layers(self, in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio, n_repeats):\n",
    "        layers = [MBConv(in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio)]\n",
    "        for _ in range(1, n_repeats):\n",
    "            layers.append(MBConv(out_channels, out_channels, kernel_size, 1, expand_ratio, se_ratio))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.head_conv(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "67e9c4e0-ea9f-4b5f-a5b1-ef1f99465447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n",
      "Total number of parameters: 4.7327M\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "model = Efficient_Net_B0(1000)\n",
    "x = model(x)\n",
    "print(x.shape)\n",
    "\n",
    "params = model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5341b1b9-ea22-451c-ad89-815944c46a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa893a-441e-4f6b-8fee-fbcadac69688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
